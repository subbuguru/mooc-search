{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9801,"sourceType":"datasetVersion","datasetId":6763},{"sourceId":3051937,"sourceType":"datasetVersion","datasetId":1868643},{"sourceId":5030350,"sourceType":"datasetVersion","datasetId":2919307},{"sourceId":9279572,"sourceType":"datasetVersion","datasetId":5456599}],"dockerImageVersionId":30396,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install -U kaleido","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:22.884723Z","iopub.execute_input":"2025-01-21T16:27:22.885157Z","iopub.status.idle":"2025-01-21T16:27:33.510158Z","shell.execute_reply.started":"2025-01-21T16:27:22.885123Z","shell.execute_reply":"2025-01-21T16:27:33.508435Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#importing libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-01-21T16:27:33.512745Z","iopub.execute_input":"2025-01-21T16:27:33.513234Z","iopub.status.idle":"2025-01-21T16:27:35.427445Z","shell.execute_reply.started":"2025-01-21T16:27:33.513196Z","shell.execute_reply":"2025-01-21T16:27:35.425862Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n![](https://i.imgur.com/NVrbtzj.jpg)\n\n\n## <b>1 <span style='color:#15C3BA'>|</span> BACKGROUND</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n\n- In this section we introduce the problem\n- Talk about where our data is from\n- What we would like to achieve in this notebook\n   \n\n### <b><span style='color:#15C3BA'> 1.1 |</span> Problem Formulation/Statement</b> \n\n- With the world becoming digital, any new skill can be acquired with just a click. However, many of us still needs a dedicated curriculum in order to excel in a specific topic\n- This is where e-learning platforms comes handy and EdX is one of such massive open online course (MOOC) providers\n- So we've found a course we like, and went through the course, so what next?\n- With the availability of so many online courses, it may be take some effort and time to look through all available courses\n- We can utilise a recommendation system to give some tips on what course the user might like to go though next\n- Whilst there are quite a number approaches to recommendation systems, well utilise an approach which requires NLP\n\n<br>\n\n### <b><span style='color:#15C3BA'> 1.2 |</span> Recommendation system</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>GOALS</span></b></p></div>\n\nThe purpose of our recommendation system is to inform a user about possible courses they make like, based on a couse they liked\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>METHOD</span></b></p></div>\n\nWe will utilise scrapped course description data (our corpus), well convert each document into vector form using (bow,embeddings), then calculate the consine similarity, from which we will be able to extract courses which are most similar \n\n<br>\n\n### <b><span style='color:#15C3BA'> 1.3 |</span> The Dataset</b> \n\n- This dataset is scraped off the publicly available information on the **EdX** website\n- This dataset consists of 720 rows and 6 columns namely Name of the Course, Name of the University, Difficulty Level, Course URL, short summary about the course and course description\n\nWhat is edX?\n\n> edX online courses are self-paced, interactive courses offered by leading universities and organizations around the world. These courses provide learners with a range of topics to explore and learn from, including computer science, business, health, engineering, humanities, and more. With edX courses, learners can gain valuable skills and knowledge in an engaging and convenient way.\n\n![](https://i.imgur.com/3oYY48C.jpg)\n\n<br>\n\n### <b><span style='color:#15C3BA'> 1.4 |</span> Notebook Goals</b> \n\nTwo subgoals are of interest:\n\n- **EDA study** | Analyse an draw conclusions based on the courses that are available\n- **Course Recommendation system** | Create a course recommendation based on a specified course ","metadata":{}},{"cell_type":"markdown","source":"## <b>2 <span style='color:#15C3BA'>|</span> idX DATASET</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n    \n- We'll read the data <code>EdX.csv</code>\n- Lower the register of column names\n- Show for one course the name, about & description \n    \n    \n### <b><span style='color:#15C3BA'> 2.1 |</span> Read Data</b> \n\nRead our data and store it in <code>data</code> & make slight adjustment to column names\n","metadata":{}},{"cell_type":"code","source":"# read data\ndata = pd.read_csv('/kaggle/input/edx-courses-dataset-2021/EdX.csv')\ndata.columns = map(str.lower, data.columns)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.429193Z","iopub.execute_input":"2025-01-21T16:27:35.429552Z","iopub.status.idle":"2025-01-21T16:27:35.509894Z","shell.execute_reply.started":"2025-01-21T16:27:35.429520Z","shell.execute_reply":"2025-01-21T16:27:35.508519Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the **corpus** we will utilise three columns **name**, **about** & **course description**","metadata":{}},{"cell_type":"code","source":"def show_course_id(id):\n    print(f\"Course Name:\\n{data['name'][id]}\",'\\n')\n    print(f\"What is the course about?:\\n{data['about'][id]}\",'\\n')\n    print(f\"Course Description:\\n{data['course description'][id]}\")\n    #see link\n    print(f\"Course Link:\\n{data['link'][id]}\")\n    \n# Show example of course description contents\nprint('Sample from Dataset:\\n')\nshow_course_id(50)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.513385Z","iopub.execute_input":"2025-01-21T16:27:35.513858Z","iopub.status.idle":"2025-01-21T16:27:35.524795Z","shell.execute_reply.started":"2025-01-21T16:27:35.513814Z","shell.execute_reply":"2025-01-21T16:27:35.523573Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 2.2 |</span> Dataset Features</b> \n\nThe features we'll be working with:\n\n- <code>Name</code> Course name\n- <code>University</code> University which offers the course\n- <code>Difficulty Level</code> The extend of how difficult the course is evaluated to be\n- <code>Link</code> HTTP link to course \n- <code>About</code> A description of what the course is about","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 2.3 |</span> Type Data / Missing Data</b> \n- All data is present & data is in <code>object</code> type","metadata":{}},{"cell_type":"code","source":"data.info()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.526625Z","iopub.execute_input":"2025-01-21T16:27:35.527113Z","iopub.status.idle":"2025-01-21T16:27:35.560827Z","shell.execute_reply.started":"2025-01-21T16:27:35.527069Z","shell.execute_reply":"2025-01-21T16:27:35.559173Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 2.4 |</span> Duplicate Rows</b> \n\n- We only have one case of identical row data, lets drop it","metadata":{}},{"cell_type":"code","source":"print(f'number of duplicate rows: {data.duplicated().sum()}')\ndata.drop_duplicates(inplace = True)\n\ndata.reset_index(drop = True, inplace = True)\nprint(f'number of rows: {data.shape[0]}')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:29:57.878385Z","iopub.execute_input":"2025-01-21T16:29:57.878927Z","iopub.status.idle":"2025-01-21T16:29:57.908594Z","shell.execute_reply.started":"2025-01-21T16:29:57.878846Z","shell.execute_reply":"2025-01-21T16:29:57.906825Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>3 <span style='color:#15C3BA'>|</span> EXPLORATORY DATA ANALYSIS</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n\n\nLet's ask ourselves some questions:\n- How many unique institutions are offering courses?\n- Which institutions offers the most courses, which offer the least?\n- What is the distribution of difficulty level of all courses?\n- What is the distribution of difficulty level of all courses for a particular institution?","metadata":{}},{"cell_type":"code","source":"print(f\"Number of unique institutions offering courses: {len(list(data['university'].value_counts().index))}\")","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.594476Z","iopub.execute_input":"2025-01-21T16:27:35.594976Z","iopub.status.idle":"2025-01-21T16:27:35.604012Z","shell.execute_reply.started":"2025-01-21T16:27:35.594896Z","shell.execute_reply":"2025-01-21T16:27:35.602368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(data['university'].value_counts().head())\ndisplay(data['university'].value_counts().tail())","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.606201Z","iopub.execute_input":"2025-01-21T16:27:35.606742Z","iopub.status.idle":"2025-01-21T16:27:35.625095Z","shell.execute_reply.started":"2025-01-21T16:27:35.606674Z","shell.execute_reply":"2025-01-21T16:27:35.623652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['difficulty level'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.627007Z","iopub.execute_input":"2025-01-21T16:27:35.627540Z","iopub.status.idle":"2025-01-21T16:27:35.639478Z","shell.execute_reply.started":"2025-01-21T16:27:35.627491Z","shell.execute_reply":"2025-01-21T16:27:35.637950Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[data['university'].str.startswith('D')]['university'].value_counts()\n\ndf_rename = {'Delft University of Technology-Wageningen University & Research-Delft University & Wageningen University':'Deft'}","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.644164Z","iopub.execute_input":"2025-01-21T16:27:35.644549Z","iopub.status.idle":"2025-01-21T16:27:35.659368Z","shell.execute_reply.started":"2025-01-21T16:27:35.644517Z","shell.execute_reply":"2025-01-21T16:27:35.657484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data['university'] = data['university'].rename(df_rename)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.660746Z","iopub.execute_input":"2025-01-21T16:27:35.661152Z","iopub.status.idle":"2025-01-21T16:27:35.680112Z","shell.execute_reply.started":"2025-01-21T16:27:35.661119Z","shell.execute_reply":"2025-01-21T16:27:35.678360Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ldf = data.groupby(['difficulty level','university'],as_index=False).size()\nldf = ldf.sort_values(by='size',ascending=False)\n\nfig = px.bar(ldf,y='university',x='size',color='difficulty level',height=900,template='plotly_white')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:35.682221Z","iopub.execute_input":"2025-01-21T16:27:35.682675Z","iopub.status.idle":"2025-01-21T16:27:37.135449Z","shell.execute_reply.started":"2025-01-21T16:27:35.682625Z","shell.execute_reply":"2025-01-21T16:27:37.133856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 3.1 |</span> n-gram of course description</b> \n\nLets check most common ngrams in description","metadata":{}},{"cell_type":"code","source":"# importing the dependencies needed for pre processing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nen_stopwords = stopwords.words(\"english\") # stop words \nlemma = WordNetLemmatizer() # lemmatiser\n\n# define a function for preprocessing\ndef clean(text):\n    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n    text = text.lower() #changes to lower case\n    tokens = word_tokenize(text) #tokenize the text\n    clean_list = [] \n    for token in tokens:\n        if token not in en_stopwords: #removes stopwords\n            clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n    return \" \".join(clean_list)# joins the tokens\n\n# applying the \"clean\" function on the text column\nldata = data['course description'].apply(clean)","metadata":{"execution":{"iopub.status.busy":"2025-01-22T16:39:58.420084Z","iopub.execute_input":"2025-01-22T16:39:58.420433Z","iopub.status.idle":"2025-01-22T16:40:00.016098Z","shell.execute_reply.started":"2025-01-22T16:39:58.420347Z","shell.execute_reply":"2025-01-22T16:40:00.014189Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/3707127841.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# applying the \"clean\" function on the text column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'course description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"],"ename":"NameError","evalue":"name 'data' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import spacy\nfrom collections import Counter\nimport plotly.express as px\n\nnlp = spacy.load('en_core_web_sm')\n\ndict_ngrams = {'unigram':[],'bigram':[],'trigram':[]}\nfor document in ldata:\n\n    doc = nlp(document)\n    tokens = [token.text for token in doc]\n\n    def n_grams(tokens,n):\n        lst_bigrams = [' '.join(i) for i in [tokens[i:i+n] for i in range(len(tokens)-n+1)]]\n        return lst_bigrams\n\n    dict_ngrams['unigram'].extend(n_grams(tokens,1))\n    dict_ngrams['bigram'].extend(n_grams(tokens,2))\n    dict_ngrams['trigram'].extend(n_grams(tokens,3))\n    \nprint('unigrams',len(dict_ngrams['bigram']))\nprint('bigrams',len(dict_ngrams['unigram']))\nprint('trigrams',len(dict_ngrams['trigram']))\n\n# plot ngrams\ndef plot_counter(counter,top,name):\n    labels, values = zip(*counter.items())\n    fig = px.bar(pd.Series(values,index=labels,name=name).sort_values(ascending=False)[:top],\n                 template='plotly_white',orientation='h')\n    fig.show('svg',dpi=300)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:27:41.044959Z","iopub.execute_input":"2025-01-21T16:27:41.045330Z","iopub.status.idle":"2025-01-21T16:28:13.709452Z","shell.execute_reply.started":"2025-01-21T16:27:41.045296Z","shell.execute_reply":"2025-01-21T16:28:13.707510Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_counter(Counter(dict_ngrams['unigram']),20,'unigram')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:13.711567Z","iopub.execute_input":"2025-01-21T16:28:13.712859Z","iopub.status.idle":"2025-01-21T16:28:15.392484Z","shell.execute_reply.started":"2025-01-21T16:28:13.712793Z","shell.execute_reply":"2025-01-21T16:28:15.390480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_counter(Counter(dict_ngrams['bigram']),20,'bigrams')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:15.394973Z","iopub.execute_input":"2025-01-21T16:28:15.395579Z","iopub.status.idle":"2025-01-21T16:28:16.277359Z","shell.execute_reply.started":"2025-01-21T16:28:15.395520Z","shell.execute_reply":"2025-01-21T16:28:16.275456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_counter(Counter(dict_ngrams['trigram']),20,'trigrams')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.279402Z","iopub.execute_input":"2025-01-21T16:28:16.279884Z","iopub.status.idle":"2025-01-21T16:28:16.572535Z","shell.execute_reply.started":"2025-01-21T16:28:16.279827Z","shell.execute_reply":"2025-01-21T16:28:16.570790Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>4 <span style='color:#15C3BA'>|</span> NATURAL LANGUAGE PROCESSING</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n\n- Remove irrelovant columns in our data that won't be utilised in this study\n- Create a new columns **text**, which will be used in our analysis \n- Do some text cleaning & stemming of the **text** column data\n- Prepare the data for both **TF-IDF** & **Word2Vec**, which require slightly different inputs","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 4.1 |</span> Drop Irrelovant Rows</b> \n\nLet's remove column data we will not utilise","metadata":{}},{"cell_type":"code","source":"data.drop(columns = [\"university\", \"difficulty level\"], \n          axis =1, inplace = True)\ndf = data.copy()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.574575Z","iopub.execute_input":"2025-01-21T16:28:16.575085Z","iopub.status.idle":"2025-01-21T16:28:16.584872Z","shell.execute_reply.started":"2025-01-21T16:28:16.575038Z","shell.execute_reply":"2025-01-21T16:28:16.583467Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 4.2 |</span> Adjust Description</b> \n\n- Create a documents which will be comprised of the **course name**, **about** & **course description**\n- We will be utilising this as our corpus data we will feed into **TF-IDF** & **Word2Vec** models\n- `data['text']` will be our corpus","metadata":{}},{"cell_type":"code","source":"data['text'] = data['name'] + ' ' + data['about'] + ' ' + data['course description']\ndata.head(5)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.586842Z","iopub.execute_input":"2025-01-21T16:28:16.587273Z","iopub.status.idle":"2025-01-21T16:28:16.616710Z","shell.execute_reply.started":"2025-01-21T16:28:16.587239Z","shell.execute_reply":"2025-01-21T16:28:16.615068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text_data = data[['name','about','course description','text']]\ntext_data.to_csv('text_data.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.618539Z","iopub.execute_input":"2025-01-21T16:28:16.618935Z","iopub.status.idle":"2025-01-21T16:28:16.691678Z","shell.execute_reply.started":"2025-01-21T16:28:16.618873Z","shell.execute_reply":"2025-01-21T16:28:16.690414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 4.3 |</span> Text Cleaning / Stemming</b> \n\nWhy is cleaning text important?\n\n- Text cleaning is an important step in NLP because it generally helps improve the accuracy of machine learning algorithms\n- With the removal of stop words (eg. and) and other unneccessary elements in a sentence, \n- It can help reduce the noise in the data & help improve the quality of the input data that is been fed into the the model\n- Additionally, it can help reduce the size of the vocabulary and size of the input data, making it faster to process\n\nConverting input words to base dictionary form:\n\n- As with text cleaning, it also help to utilise word stemmers\n- They help machine learning algorithms by normalising words to their root/dictionary form\n- What this does it help reduce the number of words (eg. player, playing, played -> play)\n- This can further improve the accuracy of the algorithms by allowing them to identify patterns in the data\n- Training time & computational resources, as a result can also be reduced","metadata":{}},{"cell_type":"code","source":"#importing the dependencies needed for pre processing\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.693452Z","iopub.execute_input":"2025-01-21T16:28:16.693799Z","iopub.status.idle":"2025-01-21T16:28:16.699396Z","shell.execute_reply.started":"2025-01-21T16:28:16.693768Z","shell.execute_reply":"2025-01-21T16:28:16.698137Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"en_stopwords = stopwords.words(\"english\") # stop words \nlemma = WordNetLemmatizer() # lemmatiser\n\n# define a function for preprocessing\ndef clean(text):\n    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n    text = text.lower() #changes to lower case\n    tokens = word_tokenize(text) #tokenize the text\n    clean_list = [] \n    for token in tokens:\n        if token not in en_stopwords: #removes stopwords\n            clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n    return \" \".join(clean_list)# joins the tokens\n\n# applying the \"clean\" function on the text column\ndata.text = data.text.apply(clean)\ndata.text","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:16.700818Z","iopub.execute_input":"2025-01-21T16:28:16.701262Z","iopub.status.idle":"2025-01-21T16:28:17.992503Z","shell.execute_reply.started":"2025-01-21T16:28:16.701211Z","shell.execute_reply":"2025-01-21T16:28:17.991143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- Let's clean & stem the pandas data frame corpus **text**","metadata":{}},{"cell_type":"code","source":"# Preprocessing, returns list instead\ndef clean_for_word2vec(text):\n    \n    text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n    text = text.lower() #changes to lower case\n    tokens = word_tokenize(text) #tokenize the text\n    clean_list = [] \n    for token in tokens:\n        if token not in en_stopwords: #removes stopwords\n            clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n    return clean_list\n\n#cleaning the documents\ncorpus_cleaned = data.text.apply(clean_for_word2vec)\nlst_corpus = corpus_cleaned.tolist()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:17.994092Z","iopub.execute_input":"2025-01-21T16:28:17.994405Z","iopub.status.idle":"2025-01-21T16:28:19.102948Z","shell.execute_reply.started":"2025-01-21T16:28:17.994378Z","shell.execute_reply":"2025-01-21T16:28:19.101477Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corpus = []\nfor words in data['text']:\n    corpus.append(words.split())\n    \nlen(f'corpus length: {corpus}')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.104696Z","iopub.execute_input":"2025-01-21T16:28:19.105173Z","iopub.status.idle":"2025-01-21T16:28:19.149351Z","shell.execute_reply.started":"2025-01-21T16:28:19.105134Z","shell.execute_reply":"2025-01-21T16:28:19.147876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>5 <span style='color:#15C3BA'>|</span> COURSE RECOMMENDATIONS</b> \n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>WHAT WE WILL DO IN THIS SECTION</span></b></p></div>\n    \n- Our approach to providing recommendations is based on **cosine similarity** of input vectors\n- The first approach we can utilise to generate vectors for each course is by utilising **Term Frequency-Inverse Document Frequency** (TF-IDF)\n- The second approach we can utilise to generate vectors for each course is by utilising **Embedding Vectors** \n    \n<br>\n    \n### <b><span style='color:#15C3BA'> 5.1 |</span> TF-IDF</b> \n    \n- First let's try utilising TF-IDF for the generation of vector representation for  our documents in the corpus <code>data['text']</code>\n- It's sufficient to input the **pandas series** into <code>TfidVectorizer</code>","metadata":{}},{"cell_type":"code","source":"# course names\nlst_names = list(data['name'])\nlst_names[:20]","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.151351Z","iopub.execute_input":"2025-01-21T16:28:19.151805Z","iopub.status.idle":"2025-01-21T16:28:19.162972Z","shell.execute_reply.started":"2025-01-21T16:28:19.151764Z","shell.execute_reply":"2025-01-21T16:28:19.161407Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>GENERATION OF VECTOR REPRESENTATION OF TEXT</span></b></p></div>\n\n- TF-IDF was described in notebook **[nlp | Natural Language Processing Reference](https://www.kaggle.com/code/shtrausslearning/nlp-natural-language-processing-reference)**\n- <code>test_matrix</code>, is input into our recommendation generation function <code>Recommendation_Cosine_similarity</code>","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(f\"fitting data on:\\n{data['text'][:3]} of type: {type(data['text'])}\")\n\nvectoriser = TfidfVectorizer()\ntest_matrix = vectoriser.fit_transform(data['text'])\nprint(f'\\noutput matrix size: {test_matrix.shape}')\nprint(f'length of the vectoriser vocabulary: {len(vectoriser.vocabulary_)}')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.164877Z","iopub.execute_input":"2025-01-21T16:28:19.165390Z","iopub.status.idle":"2025-01-21T16:28:19.309019Z","shell.execute_reply.started":"2025-01-21T16:28:19.165351Z","shell.execute_reply":"2025-01-21T16:28:19.307201Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>GENERATION OF VECTOR REPRESENTATION OF TEXT</span></b></p></div>\n\nExample recommendation for: **MathTrackX: Differential Calculus**\n\n> - 1 MathTrackX: Differential Calculus\n> - 2 MathTrackX: Integral Calculus\n> - 3 MathTrackX: Statistics\n> - 4 MathTrackX: Polynomials, Functions and Graphs\n> - 5 MathTrackX: Probability\n> - 6 MathTrackX: Special Functions\n\n\n","metadata":{}},{"cell_type":"code","source":"# define a function that will return the first five recommended courses\ndef Recommendation_Cosine_similarity(matrix, name):\n    \n    # get its index from list\n    row_num = lst_names.index(name)\n     \n    # cosine similarity matrix for each index in list (square matrix)\n    similarity = cosine_similarity(test_matrix)\n    \n    # get similar courses by highest cosine similarity\n    similar_courses = list(enumerate(similarity[row_num]))\n    sorted_similar_courses = sorted(similar_courses, key=lambda x:x[1], reverse= True)[:6]\n    \n    print(f'recommended courses for {name}\\n')\n    # This part will return the description of the recommended courses\n    i = 0\n    for item in sorted_similar_courses:\n        course_description = data[data.index == item[0]][\"name\"].values[0]\n        recommendations = print(f\"{i+1} {course_description}\")\n        i = i + 1\n    return recommendations\n\nRecommendation_Cosine_similarity(test_matrix,'MathTrackX: Differential Calculus')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.311320Z","iopub.execute_input":"2025-01-21T16:28:19.312100Z","iopub.status.idle":"2025-01-21T16:28:19.377629Z","shell.execute_reply.started":"2025-01-21T16:28:19.312034Z","shell.execute_reply":"2025-01-21T16:28:19.375334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 5.2 |</span> Word2Vec w/ Gensim</b>\n\nGensim is a very useful library, let's look at some basic aspects of it, hopefully you will quickly grasp how to use it\n\n<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>EMBEDDING USAGE EXAMPLES</span></b></p></div>\n\n- Load GoogleNews embedding vectors (already vectors, not model.wv -> just model gives **embedding vectors**)\n- Existing vectors arent trainable & can only be used as it is for embedding extraction of specific words\n- We can preload existing word embedding vectors from files (eg. GoogleNews) using <code>KeyedVectors</code> \n- Which is not to be confused with <code>Word2Vec</code>, before version 4, we could have utilised pretrained KeyedVectors by merging them with <code>Word2Vec</code>","metadata":{}},{"cell_type":"code","source":"import gensim.downloader as api\nfrom gensim.models import KeyedVectors\n\n# preloaded embedding vectors\n# word2vec_path = '/kaggle/input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n# w2v_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.385150Z","iopub.execute_input":"2025-01-21T16:28:19.385595Z","iopub.status.idle":"2025-01-21T16:28:19.647339Z","shell.execute_reply.started":"2025-01-21T16:28:19.385557Z","shell.execute_reply":"2025-01-21T16:28:19.645777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# '''\n#               Some operations we can do with KeyedVectors\n# '''\n\n# result = w2v_model.most_similar(positive=['woman', 'king'], negative=['man'])\n# print(result)\n\n# result = w2v_model.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n# print(result)\n\n# print(w2v_model.doesnt_match(\"breakfast cereal dinner lunch\".split()))\n\n# similarity = w2v_model.similarity('woman', 'man')\n# print(similarity)\n\n# result = w2v_model.similar_by_word(\"cat\")\n# print(result)\n\n# # embedding vectors\n\n# vector = w2v_model['computer']  # numpy vector of a word\n# # print(vector[:10])\n\n# vector = w2v_model.get_vector('office', norm=True)\n\n# for i in vector[:10]:\n#     print(i)\n# # print(vector[:10])","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.649338Z","iopub.execute_input":"2025-01-21T16:28:19.649784Z","iopub.status.idle":"2025-01-21T16:28:19.656721Z","shell.execute_reply.started":"2025-01-21T16:28:19.649748Z","shell.execute_reply":"2025-01-21T16:28:19.655018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>TRAINING OUR WORD2VEC MODEL</span></b></p></div>\n\n- Unlike <code>KeyedVectors</code>, we can train the <code>Word2Vec</code> model\n- LEt's create word embedding vectors for each word in the corpus **text**","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nmodel = Word2Vec(vector_size=100,min_count=1)\nmodel.build_vocab(corpus)\nprint(f\"words in corpus: {model.corpus_total_words}\")\nprint(f'corpus count: {model.corpus_count}')\nmodel.train(corpus, total_examples = model.corpus_count, epochs = 50)\nmodel.save('embeddings')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:19.658641Z","iopub.execute_input":"2025-01-21T16:28:19.659137Z","iopub.status.idle":"2025-01-21T16:28:27.288088Z","shell.execute_reply.started":"2025-01-21T16:28:19.659092Z","shell.execute_reply":"2025-01-21T16:28:27.286523Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>TESTING THE MODEL</span></b></p></div>\n\n- As we did above in the <code>KeyedVectors</code> part, let's check some relations between different words ","metadata":{}},{"cell_type":"code","source":"vocab_len = len(model.wv)\nprint(f'Vocabulary size: {vocab_len}')\n\nprint('First 10 words in vocabulary:')\nkey_vocab = model.wv.index_to_key[:10]\nprint(key_vocab)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:27.289694Z","iopub.execute_input":"2025-01-21T16:28:27.290226Z","iopub.status.idle":"2025-01-21T16:28:27.298449Z","shell.execute_reply.started":"2025-01-21T16:28:27.290179Z","shell.execute_reply":"2025-01-21T16:28:27.296803Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"result = model.wv.similar_by_word('deep')\nfor i in result:\n    print(i)\n\nword1 = 'deep'; word2 = 'learning'\nsimilarity = model.wv.similarity(word1,word2)\nprint(f'\\nsimilarity b/w {word1} and {word2} {round(similarity,2)}\\n')\n\n# embedding vectors\n\nvector = model.wv['computer']  # numpy vector of a word\nprint(f'computer word embedding')\nprint(f'first {10} components')\nprint(vector[:10])","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:27.300363Z","iopub.execute_input":"2025-01-21T16:28:27.300820Z","iopub.status.idle":"2025-01-21T16:28:27.338106Z","shell.execute_reply.started":"2025-01-21T16:28:27.300777Z","shell.execute_reply":"2025-01-21T16:28:27.331544Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# View similar words based on gensim's model\nprint('Similar Words')\nsimilar_words = {search_term: [item[0] for item in model.wv.most_similar([search_term], topn=5)]\n                  for search_term in key_vocab}\nsimilar_words","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:27.342971Z","iopub.execute_input":"2025-01-21T16:28:27.345306Z","iopub.status.idle":"2025-01-21T16:28:27.379947Z","shell.execute_reply.started":"2025-01-21T16:28:27.345177Z","shell.execute_reply":"2025-01-21T16:28:27.377136Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>VISUALISATION EMBEDDING VECTORS</span></b></p></div>\n\n- Embedding vectors encode information about a word in a very high dimensional space, let's utilise <code>TSNE</code>, to reduce the number of dimensions\n- The resulting two dimensional array data can then be visualised, so we can understand similarity between words embeddings","metadata":{}},{"cell_type":"code","source":"# Lower dimensionality visualisation of embeddings (100->2)\nimport plotly.express as px\nfrom sklearn.manifold import TSNE\nimport warnings; warnings.filterwarnings('ignore')\n\nwords = sum([[k] + v for k, v in similar_words.items()], [])\nwvs = model.wv[words]\n\ntsne = TSNE(n_components=2, \n            random_state=0, \n            n_iter=10000)\n\nX = tsne.fit_transform(wvs)\nlabels = words\n    \nfig = px.scatter(X[:, 0], X[:, 1],text=labels,\n           template='plotly_white',\n           width=800,\n           title='Word Embedding Visualisation')\nfig.show('svg',dpi=300)","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:28:27.383520Z","iopub.execute_input":"2025-01-21T16:28:27.386693Z","iopub.status.idle":"2025-01-21T16:29:57.207540Z","shell.execute_reply.started":"2025-01-21T16:28:27.386601Z","shell.execute_reply":"2025-01-21T16:29:57.206030Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"color:white;display:fill;border-radius:8px;font-size:100%; letter-spacing:1.0px;\"><p style=\"padding: 5px;color:white;text-align:left;\"><b><span style='color:#15C3BA'>AVERAGE SENTENCE EMBEDDINGS</span></b></p></div>\n\n- Each document is made up of multiple tokens, we can utilise a commonly used approach to merge the embedding vectors\n- The combined embeddings will create a single sentence embedding, which will still make sense","metadata":{}},{"cell_type":"code","source":"# Get average embedding vector for each text\ndef doc_vectorizer(doc, model):\n    \n    doc_vector = []\n    num_words = 0\n    \n    for word in doc:\n        try:\n            if num_words == 0:\n                doc_vector = model.wv[word]\n            else:\n                doc_vector = np.add(doc_vector, model.wv[word])\n            num_words += 1\n        except:\n            pass  # if embedding vector isn't found\n     \n    return np.asarray(doc_vector) / num_words\n\nX = []\nfor doc in lst_corpus:\n    X.append(doc_vectorizer(doc,model))\n    \nprint(f'list of sentence vectors/sentences: {len(X)}')\nprint(f'each sentence has {X[0].shape} dimensions')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:29:57.209336Z","iopub.execute_input":"2025-01-21T16:29:57.209771Z","iopub.status.idle":"2025-01-21T16:29:57.450369Z","shell.execute_reply.started":"2025-01-21T16:29:57.209728Z","shell.execute_reply":"2025-01-21T16:29:57.448605Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### <b><span style='color:#15C3BA'> 5.3 |</span> Recommendation using cosine similarity</b>\n\nExample recommendation for: **Data Science: Inference and Modeling**\n\n> - Introduction to Statistical Methods for Gene Mapping\n> - Data Science: Inferential Thinking through Simulations\n> - Data Science: R Basics\n> - Probability and Statistics in Data Science using Python\n> - Data Science: Linear Regression","metadata":{}},{"cell_type":"code","source":"def course_recommender(X,course):\n\n    # Finding cosine similarity for the vectors\n    cosine_similarities = cosine_similarity(X,X)\n\n    # Taking the Title and Movie Image Link and store in new dataframe called 'movies'\n    courses = data[['name']]\n\n    # Reverse mapping of the index\n    indices = pd.Series(data.index, index = data['name']).drop_duplicates()\n\n    idx = indices[course]\n    sim_scores = list(enumerate(cosine_similarities[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n    sim_scores = sim_scores[1:6]\n    movie_indices = [i[0] for i in sim_scores]\n    recommend = courses.iloc[movie_indices]\n\n    for index, row in recommend.iterrows():\n        print(row['name'])\n        \ncourse_recommender(X,'Data Science: Inference and Modeling')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:29:57.452085Z","iopub.execute_input":"2025-01-21T16:29:57.452481Z","iopub.status.idle":"2025-01-21T16:29:57.498694Z","shell.execute_reply.started":"2025-01-21T16:29:57.452449Z","shell.execute_reply":"2025-01-21T16:29:57.493982Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## <b>6 <span style='color:#15C3BA'>|</span> PUTTING TOGETHER RECOMMENDER CLASS</b> \n\nLets put together everything above into a single class, so the its much more userfriends and organised","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncorpus = pd.read_csv('/kaggle/input/edx-textdata/text_data.csv')\ncorpus.head()","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:29:57.500488Z","iopub.execute_input":"2025-01-21T16:29:57.503559Z","iopub.status.idle":"2025-01-21T16:29:57.626620Z","shell.execute_reply.started":"2025-01-21T16:29:57.503488Z","shell.execute_reply":"2025-01-21T16:29:57.625134Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#get corpus as text_data.csv\n\nclass recommender:\n    \n    def __init__(self,corpus):\n        self.corpus = corpus # corpus dataframe\n        self.course_names = list(data['name']) # course names only\n\n        \n    '''\n    \n    Text Clearning for Corpus\n    \n    '''\n        \n    @staticmethod\n    def clean_for_word2vec(text):\n\n        text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n        text = text.lower() #changes to lower case\n        tokens = word_tokenize(text) #tokenize the text\n        clean_list = [] \n        for token in tokens:\n            if token not in en_stopwords: #removes stopwords\n                clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n        return clean_list\n    \n    @staticmethod\n    def clean(text):\n        text = re.sub(\"[^A-Za-z1-9 ]\", \"\", text) #removes punctuation marks\n        text = text.lower() #changes to lower case\n        tokens = word_tokenize(text) #tokenize the text\n        clean_list = [] \n        for token in tokens:\n            if token not in en_stopwords: #removes stopwords\n                clean_list.append(lemma.lemmatize(token)) #lemmatizing and appends to clean_list\n        return \" \".join(clean_list)# joins the tokens\n\n    \n    # create average emebedding vector for sentence\n    \n    @staticmethod\n    def doc_vectorizer(doc, model):\n\n        doc_vector = []\n        num_words = 0\n\n        for word in doc:\n            try:\n                if num_words == 0:\n                    doc_vector = model.wv[word]\n                else:\n                    doc_vector = np.add(doc_vector, model.wv[word])\n                num_words += 1\n            except:\n                pass  # if embedding vector isn't found\n\n        return np.asarray(doc_vector) / num_words\n    \n    \n    # cosine similarity for tfidf\n    \n    def course_recommender_tfidf(self,matrix, name):\n\n        # get its index from list\n        row_num = self.course_names.index(name)\n\n        # cosine similarity matrix for each index in list (square matrix)\n        similarity = cosine_similarity(test_matrix)\n\n        # get similar courses by highest cosine similarity\n        similar_courses = list(enumerate(similarity[row_num]))\n        sorted_similar_courses = sorted(similar_courses, key=lambda x:x[1], reverse= True)[:6]\n\n        print(f'Recommended courses for \\n{name}\\n')\n        # This part will return the description of the recommended courses\n        i = 0\n        for item in sorted_similar_courses:\n            course_description = self.corpus[self.corpus.index == item[0]][\"name\"].values[0]\n            recommendations = print(f\"{i+1} {course_description}\")\n            i = i + 1\n        return recommendations\n    \n    # cosine similarity for word2vec\n\n    def course_recommender_w2v(X,course): #w2v would be better because it goes beyond direct keyword matching and captures semantic meaning. cons are\n        #requires a large corpus to be trained on which is alleviated because we use the google word2vec dataset\n\n        # Finding cosine similarity for the vectors\n        cosine_similarities = cosine_similarity(X,X)\n\n        # Taking the Title and Movie Image Link and store in new dataframe called 'movies'\n        courses = data[['name']]\n\n        # Reverse mapping of the index\n        indices = pd.Series(data.index, index = data['name']).drop_duplicates()\n\n        idx = indices[course]\n        sim_scores = list(enumerate(cosine_similarities[idx]))\n        sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n        sim_scores = sim_scores[1:6]\n        movie_indices = [i[0] for i in sim_scores]\n        recommend = courses.iloc[movie_indices]\n\n        for index, row in recommend.iterrows():\n            print(row['name'])\n            \n    def tfidf_recommend(self,course):\n        \n        vectoriser = TfidfVectorizer()\n        self.test_matrix = vectoriser.fit_transform(self.corpus['text'])\n        self.course_recommender_tfidf(test_matrix,course)\n        \n        \n    # input course currently taking\n            \n    def __call__(self,inputs):\n        \n        recommendations = self.tfidf_recommend(inputs)\n\n# input corpus\nrecomm = recommender(corpus)\nrecomm('Roblox')","metadata":{"execution":{"iopub.status.busy":"2025-01-21T16:55:01.037481Z","iopub.execute_input":"2025-01-21T16:55:01.039703Z","iopub.status.idle":"2025-01-21T16:55:02.026332Z","shell.execute_reply.started":"2025-01-21T16:55:01.039621Z","shell.execute_reply":"2025-01-21T16:55:02.023991Z"},"trusted":true},"outputs":[],"execution_count":null}]}