{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3051937,"sourceType":"datasetVersion","datasetId":1868643},{"sourceId":5103339,"sourceType":"datasetVersion","datasetId":2423448},{"sourceId":9279572,"sourceType":"datasetVersion","datasetId":5456599},{"sourceId":9976645,"sourceType":"datasetVersion","datasetId":6138541},{"sourceId":256574,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":204042,"modelId":225262}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup\n\n# Install NLTK and other packages\n!pip list | grep nltk\n! pip install -U kaleido\n!pip install sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re\nimport nltk\n\n\n\n\nnltk.download('punkt')  \nnltk.download('wordnet')  \n\n# Unzip per this stackoverflow: https://stackoverflow.com/questions/73849624/getting-error-while-submitting-notebook-on-kaggle-even-after-importing-nltk-libr\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_uuid":"11872ad3-929a-453c-a9a8-212f3f47cff9","_cell_guid":"ad38c9bc-2362-4c5f-bb22-04197c45c913","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:26:27.723111Z","iopub.execute_input":"2025-03-05T16:26:27.723356Z","iopub.status.idle":"2025-03-05T16:27:26.334565Z","shell.execute_reply.started":"2025-03-05T16:26:27.723335Z","shell.execute_reply":"2025-03-05T16:27:26.333356Z"}},"outputs":[{"name":"stdout","text":"nltk                               3.2.4\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Data cleaning and normalization\n\n# Normalize/clean course data to the name, topic, link, text format for now\n\ndataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\ndataMit.columns = map(str.lower, dataMit.columns)\ndataMit.rename(columns={'name ': 'name'}, inplace=True)\ndataMit.rename(columns={'course link': 'link'}, inplace=True)\ndataMit['text'] = dataMit['name'] + \" \" + dataMit['topic'] \ndataMit['provider'] = 'Massachussets Institute of Technology'\ndataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\ndataHarvard.columns = map(str.lower, dataHarvard.columns)\ndataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\ndataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\ndataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic'] \ndataHarvard['provider'] = 'Harvard University'\ndataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\ndataEdx.columns = map(str.lower, dataEdx.columns)\ndataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\ndataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\ndataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\ndataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\ndataUdemy.columns = map(str.lower, dataUdemy.columns)\ndataUdemy.rename(columns={\n    'title': 'name',\n    'headline': 'topic',\n    'url': 'link',\n}, inplace=True)\n# only keep free courses\ndataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n# Since Udemy courses are user generated, filter only courses with rating over 4.5\ndataUdemy['provider'] = 'Udemy'\ndataUdemy = dataUdemy[dataUdemy['rating'] > 4.5 ]\ndataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\ndataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\ndataCoursera.rename(columns={\n    'title': 'name',\n    'skills': 'topic',\n    'url': 'link',\n}, inplace=True)\ndataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\ndataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n\ndataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\ndataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]","metadata":{"_uuid":"96a00073-5fdf-4e9c-8ae4-a2aac33628dd","_cell_guid":"c5885aea-ad3a-4df3-8c01-65d9d1225cf1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:27:36.685389Z","iopub.execute_input":"2025-03-05T16:27:36.685754Z","iopub.status.idle":"2025-03-05T16:27:41.929450Z","shell.execute_reply.started":"2025-03-05T16:27:36.685721Z","shell.execute_reply":"2025-03-05T16:27:41.928464Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def clean_text(text):\n    lemma = WordNetLemmatizer() # lemmatizer\n    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n    text = text.lower()\n    tokens = word_tokenize(text) # look into this tokenization\n    tokens = [lemma.lemmatize(word) for word in tokens # lemmatize words and remove stopwords \n                if word not in stopwords.words(\"english\")]\n    return \" \".join(tokens) # SBERT rrequires joined tokens\n\n#Combine and clean data\ndata = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\ndata['cleaned_text'] = data['text'].apply(clean_text) # Add clean text column to dataframe\n\n# Drop non-english courses\nindices_to_drop = [index for index, row in data.iterrows() if bool(re.search(r'[^\\x00-\\x7F\\u2000-\\u206F\\u2600-\\u26FF\\u2700-\\u27BF]', str(row['text'])))]\ndata = data.drop(indices_to_drop)\n\ndata.head()","metadata":{"_uuid":"703c98e4-4486-4f12-a888-a3ffb4061644","_cell_guid":"5db69b4c-1811-4507-8f4c-41fa19d87acc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:27:48.148697Z","iopub.execute_input":"2025-03-05T16:27:48.149011Z","iopub.status.idle":"2025-03-05T16:28:14.353534Z","shell.execute_reply.started":"2025-03-05T16:27:48.148989Z","shell.execute_reply":"2025-03-05T16:28:14.352657Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                    name  \\\n26443                           Stock Market Foundations   \n26445  The Complete Course On Understanding Blockchai...   \n26446  Bitcoin or How I Learned to Stop Worrying and ...   \n26448  Blockchain cryptocurrency course 101 for absol...   \n26449  Trading Options For Consistent Returns: Option...   \n\n                                                   topic  \\\n26443  The Market isn't a Mystery, It’s a Playground....   \n26445  A Beginner's Guide to Authentic Knowledge on B...   \n26446  The definitive guide to understand what the bi...   \n26448  A complete guide to anyone who wants to really...   \n26449                     The Foundation For Consistency   \n\n                                                    link provider  \\\n26443  https://www.udemy.com/course/how-to-invest-in-...    Udemy   \n26445  https://www.udemy.com/course/understanding-blo...    Udemy   \n26446  https://www.udemy.com/course/bitcoin-or-how-i-...    Udemy   \n26448  https://www.udemy.com/course/blockchain-crypto...    Udemy   \n26449  https://www.udemy.com/course/trading-options-f...    Udemy   \n\n                                                    text  \\\n26443  Stock Market Foundations The Market isn't a My...   \n26445  The Complete Course On Understanding Blockchai...   \n26446  Bitcoin or How I Learned to Stop Worrying and ...   \n26448  Blockchain cryptocurrency course 101 for absol...   \n26449  Trading Options For Consistent Returns: Option...   \n\n                                            cleaned_text  \n26443  stock market foundation market isnt mystery pl...  \n26445  complete course understanding blockchain techn...  \n26446  bitcoin learned stop worrying love crypto defi...  \n26448  blockchain cryptocurrency course 101 absolute ...  \n26449  trading option consistent return option basic ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>topic</th>\n      <th>link</th>\n      <th>provider</th>\n      <th>text</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26443</th>\n      <td>Stock Market Foundations</td>\n      <td>The Market isn't a Mystery, It’s a Playground....</td>\n      <td>https://www.udemy.com/course/how-to-invest-in-...</td>\n      <td>Udemy</td>\n      <td>Stock Market Foundations The Market isn't a My...</td>\n      <td>stock market foundation market isnt mystery pl...</td>\n    </tr>\n    <tr>\n      <th>26445</th>\n      <td>The Complete Course On Understanding Blockchai...</td>\n      <td>A Beginner's Guide to Authentic Knowledge on B...</td>\n      <td>https://www.udemy.com/course/understanding-blo...</td>\n      <td>Udemy</td>\n      <td>The Complete Course On Understanding Blockchai...</td>\n      <td>complete course understanding blockchain techn...</td>\n    </tr>\n    <tr>\n      <th>26446</th>\n      <td>Bitcoin or How I Learned to Stop Worrying and ...</td>\n      <td>The definitive guide to understand what the bi...</td>\n      <td>https://www.udemy.com/course/bitcoin-or-how-i-...</td>\n      <td>Udemy</td>\n      <td>Bitcoin or How I Learned to Stop Worrying and ...</td>\n      <td>bitcoin learned stop worrying love crypto defi...</td>\n    </tr>\n    <tr>\n      <th>26448</th>\n      <td>Blockchain cryptocurrency course 101 for absol...</td>\n      <td>A complete guide to anyone who wants to really...</td>\n      <td>https://www.udemy.com/course/blockchain-crypto...</td>\n      <td>Udemy</td>\n      <td>Blockchain cryptocurrency course 101 for absol...</td>\n      <td>blockchain cryptocurrency course 101 absolute ...</td>\n    </tr>\n    <tr>\n      <th>26449</th>\n      <td>Trading Options For Consistent Returns: Option...</td>\n      <td>The Foundation For Consistency</td>\n      <td>https://www.udemy.com/course/trading-options-f...</td>\n      <td>Udemy</td>\n      <td>Trading Options For Consistent Returns: Option...</td>\n      <td>trading option consistent return option basic ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Initialize the model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Get a list of the document embedding vector for each sentence in the cleaned text data. The indices will be aligned with the original course rows in dataframe\ndocument_embeddings = model.encode(data['cleaned_text'].tolist())","metadata":{"_uuid":"413030c0-e87c-45cb-8889-5563c6bcd487","_cell_guid":"c6791a3a-7829-469a-906e-710b1bad80e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:28:19.235361Z","iopub.execute_input":"2025-03-05T16:28:19.235795Z","iopub.status.idle":"2025-03-05T16:28:32.200511Z","shell.execute_reply.started":"2025-03-05T16:28:19.235759Z","shell.execute_reply":"2025-03-05T16:28:32.199851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71be5a574a624958a617ee35f412928a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6fd9ab0198046c29d4f7e35d7563b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dabaf26f9df48ea8c65d469822ee641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff094b687299407a93f9b1aab4bafc3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b559b96b50241669fc2a1ae7334441e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc8bf00eae341859c17936739a9ac54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a6d112e9244466995937f383814476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4870095cfc9944578560768d600753e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb2215264e54b8a8112aa9c5c5defbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3e171b61bd4bdf9d9ac70e6669ea64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6109af19c2e841d68fff395405ec6694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e759a1e5af49328c82b5275f622c40"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Export a csv with embeddings for fastapi\ndata = data[['name', 'topic', 'link', 'provider']]\ndata.to_csv('courses.csv', index=False)\nembeddings = pd.DataFrame(document_embeddings)\nembeddings.to_csv('embeddings.csv', index=False)","metadata":{"_uuid":"c71d32e2-0714-4827-8ce0-5241da0eb99b","_cell_guid":"1d152a10-a5f2-4f22-bed4-89003c49dbb7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-05T16:28:37.946995Z","iopub.execute_input":"2025-03-05T16:28:37.947311Z","iopub.status.idle":"2025-03-05T16:28:39.829861Z","shell.execute_reply.started":"2025-03-05T16:28:37.947286Z","shell.execute_reply":"2025-03-05T16:28:39.829072Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Use previous functions to process user input into vector and use cosine \n# Cosine Similarity to find the most related courses\ndef recommend_courses(user_input, document_embeddings, data, model, top_n=5):\n    cleaned_input = clean_text(user_input)\n    input_embedding = model.encode([cleaned_input]) # Model must be initialized\n    similarities = cosine_similarity(input_embedding, document_embeddings)[0]\n    top_indices = np.argsort(similarities)[-top_n:][::-1]\n    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n    return recommendations\n\n\nuser_input = \"Chaucer and Middle English Literature\"\nrecommendations = recommend_courses(user_input, document_embeddings, data, model)\nrecommendations.head()","metadata":{"_uuid":"35733aa0-cfa5-4906-8acf-1cc3ce6e03d0","_cell_guid":"75e01ceb-702b-4b16-8229-f09e027434e9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:28:44.163922Z","iopub.execute_input":"2025-03-05T16:28:44.164277Z","iopub.status.idle":"2025-03-05T16:28:44.243481Z","shell.execute_reply.started":"2025-03-05T16:28:44.164235Z","shell.execute_reply":"2025-03-05T16:28:44.242552Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08b197007303411ea5e21647515d4683"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                                                   name  \\\n2356     Medieval Literature: Dante, Boccaccio, Chaucer   \n1350  Eighteenth-Century Literature: Versions of the...   \n1331  Eighteenth-Century Literature: Versions of the...   \n1733            Literary Studies: The Legacy of England   \n792                             Old English and Beowulf   \n\n                                  topic  \\\n2356    Humanities, Literature, History   \n1350    Humanities, Literature, History   \n1331    Humanities, Literature, History   \n1733  Humanities, Literature, Criticism   \n792    Humanities, Literature, Language   \n\n                                                   link  \\\n2356  https://ocw.mit.edu/courses/21l-460-medieval-l...   \n1350  https://ocw.mit.edu/courses/21l-470-eighteenth...   \n1331  https://ocw.mit.edu/courses/21l-470-eighteenth...   \n1733  https://ocw.mit.edu/courses/21l-420-literary-s...   \n792   https://ocw.mit.edu/courses/21l-601j-old-engli...   \n\n                                   provider  \n2356  Massachussets Institute of Technology  \n1350  Massachussets Institute of Technology  \n1331  Massachussets Institute of Technology  \n1733  Massachussets Institute of Technology  \n792   Massachussets Institute of Technology  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>topic</th>\n      <th>link</th>\n      <th>provider</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2356</th>\n      <td>Medieval Literature: Dante, Boccaccio, Chaucer</td>\n      <td>Humanities, Literature, History</td>\n      <td>https://ocw.mit.edu/courses/21l-460-medieval-l...</td>\n      <td>Massachussets Institute of Technology</td>\n    </tr>\n    <tr>\n      <th>1350</th>\n      <td>Eighteenth-Century Literature: Versions of the...</td>\n      <td>Humanities, Literature, History</td>\n      <td>https://ocw.mit.edu/courses/21l-470-eighteenth...</td>\n      <td>Massachussets Institute of Technology</td>\n    </tr>\n    <tr>\n      <th>1331</th>\n      <td>Eighteenth-Century Literature: Versions of the...</td>\n      <td>Humanities, Literature, History</td>\n      <td>https://ocw.mit.edu/courses/21l-470-eighteenth...</td>\n      <td>Massachussets Institute of Technology</td>\n    </tr>\n    <tr>\n      <th>1733</th>\n      <td>Literary Studies: The Legacy of England</td>\n      <td>Humanities, Literature, Criticism</td>\n      <td>https://ocw.mit.edu/courses/21l-420-literary-s...</td>\n      <td>Massachussets Institute of Technology</td>\n    </tr>\n    <tr>\n      <th>792</th>\n      <td>Old English and Beowulf</td>\n      <td>Humanities, Literature, Language</td>\n      <td>https://ocw.mit.edu/courses/21l-601j-old-engli...</td>\n      <td>Massachussets Institute of Technology</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Testing qwen workflow setup\n# For Qwen 1.5b inference\n\n# Imports + installs\n\nimport gc\nimport torch\nfrom IPython.display import display, Markdown, Latex, HTML\nimport time\nimport re\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n!pip install mistletoe\nimport mistletoe\n\ntorch.cuda.empty_cache()\ngc.collect()\n\ntorch.cuda.empty_cache()  # Clears unused cached memory\ntorch.cuda.ipc_collect()  # Collects unused memory\n\nprint(\"Using GPU:\", torch.cuda.get_device_name(0))\nprint(f'\\n\\nMemory Usage:')\nprint('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\nprint('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inference LLM/setup\n\n\n# Load the Qwen 1.5b model\nmodel_name = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\"\nmodel_qwen = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define the Qwen query function\ndef ask_model(system=\"You are a search query optimizer.\", prompt=\"Optimize this search query:\"):\n    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_qwen.device)\n    generated_ids = model_qwen.generate(**model_inputs, max_new_tokens=3000, pad_token_id=tokenizer.eos_token_id)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\n# Example usage\nuser_input = \"Chaucer and Middle English Literature\"\noptimized_query = ask_model(prompt=f\"Optimize this search query: {user_input}\")\nrecommendations = recommend_courses(optimized_query, document_embeddings, data, model_bert)\nprint(recommendations)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:29:03.099405Z","iopub.execute_input":"2025-03-05T16:29:03.099911Z","iopub.status.idle":"2025-03-05T16:29:03.286098Z","shell.execute_reply.started":"2025-03-05T16:29:03.099873Z","shell.execute_reply":"2025-03-05T16:29:03.284980Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a3156d41d0f4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmistletoe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the Qwen 1.5b model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mistletoe'"],"ename":"ModuleNotFoundError","evalue":"No module named 'mistletoe'","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"* https://huggingface.co/docs/transformers/en/model_doc/bert","metadata":{"_uuid":"79ca17d0-35bb-44de-953f-ba1de3a529d6","_cell_guid":"16751f0b-0b52-4c3f-ac9f-d971f41b0b87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}