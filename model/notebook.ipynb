{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3051937,"sourceType":"datasetVersion","datasetId":1868643},{"sourceId":5103339,"sourceType":"datasetVersion","datasetId":2423448},{"sourceId":9279572,"sourceType":"datasetVersion","datasetId":5456599},{"sourceId":9976645,"sourceType":"datasetVersion","datasetId":6138541}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup\n\n# Install NLTK and other packages\n!pip list | grep nltk\n! pip install -U kaleido\n!pip install sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re\nimport nltk\n\n\n\n\nnltk.download('punkt')  \nnltk.download('wordnet')  \n\n# Unzip per this stackoverflow: https://stackoverflow.com/questions/73849624/getting-error-while-submitting-notebook-on-kaggle-even-after-importing-nltk-libr\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_uuid":"eda72c65-734b-4f3d-b1eb-34c11c5a76a4","_cell_guid":"12a1f766-ab6a-49ea-a6a0-526d4683ad7f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data cleaning and normalization\n\n# Normalize/clean course data to the name, topic, link, text format for now\n\ndataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\ndataMit.columns = map(str.lower, dataMit.columns)\ndataMit.rename(columns={'name ': 'name'}, inplace=True)\ndataMit.rename(columns={'course link': 'link'}, inplace=True)\ndataMit['text'] = dataMit['name'] + \" \" + dataMit['topic'] \ndataMit['provider'] = 'Massachussets Institute of Technology'\ndataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\ndataHarvard.columns = map(str.lower, dataHarvard.columns)\ndataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\ndataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\ndataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic'] \ndataHarvard['provider'] = 'Harvard University'\ndataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\ndataEdx.columns = map(str.lower, dataEdx.columns)\ndataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\ndataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\ndataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\ndataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\ndataUdemy.columns = map(str.lower, dataUdemy.columns)\ndataUdemy.rename(columns={\n    'title': 'name',\n    'headline': 'topic',\n    'url': 'link',\n}, inplace=True)\n# only keep free courses\ndataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n# Since Udemy courses are user generated, filter only courses with rating over 4.5\ndataUdemy['provider'] = 'Udemy'\ndataUdemy = dataUdemy[dataUdemy['rating'] > 4.5 ]\ndataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\ndataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\ndataCoursera.rename(columns={\n    'title': 'name',\n    'skills': 'topic',\n    'url': 'link',\n}, inplace=True)\ndataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\ndataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n\ndataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\ndataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]\ndataCoursera = dataCoursera.fillna(\"\") # Fill null values","metadata":{"_uuid":"bd5efc82-6a92-419d-98b3-61cd17469dc4","_cell_guid":"72ae4535-ce40-4499-b65d-251e7e95424f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean_text(text):\n    lemma = WordNetLemmatizer() # lemmatizer\n    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n    text = text.lower()\n    tokens = word_tokenize(text) # look into this tokenization\n    tokens = [lemma.lemmatize(word) for word in tokens # lemmatize words and remove stopwords \n                if word not in stopwords.words(\"english\")]\n    return \" \".join(tokens) # SBERT rrequires joined tokens\n\n#Combine and clean data\ndata = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\ndata['cleaned_text'] = data['text'].apply(clean_text) # Add clean text column to dataframe\ndata.head()","metadata":{"_uuid":"069d05a5-e8b9-4a84-a5aa-04a16a4558c1","_cell_guid":"8f78e3e1-5ef8-481b-b01f-5127b7269c34","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Get a list of the document embedding vector for each sentence in the cleaned text data. The indices will be aligned with the original course rows in dataframe\ndocument_embeddings = model.encode(data['cleaned_text'].tolist())","metadata":{"_uuid":"38d62f84-a535-489f-bb6e-8a19d7002649","_cell_guid":"53b19cf2-433d-4de7-846e-916b2d49b075","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use previous functions to process user input into vector and use cosine \n# Cosine Similarity to find the most related courses\ndef recommend_courses(user_input, document_embeddings, data, model, top_n=5):\n    cleaned_input = clean_text(user_input)\n    input_embedding = model.encode([cleaned_input]) # Model must be initialized\n    similarities = cosine_similarity(input_embedding, document_embeddings)[0]\n    top_indices = np.argsort(similarities)[-top_n:][::-1]\n    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n    return recommendations\n\n\nuser_input = \"Chaucer and Middle English Literature\"\nrecommendations = recommend_courses(user_input, document_embeddings, data, model)\nrecommendations.head()","metadata":{"_uuid":"78e5b0c4-4273-455e-bc9b-1a2326cb7888","_cell_guid":"87884930-300c-4617-8b40-3818416894c3","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"* https://huggingface.co/docs/transformers/en/model_doc/bert","metadata":{"_uuid":"e64a2335-efe7-4841-bc81-fb838c2fb0eb","_cell_guid":"510fad14-8ffb-4bd3-87f9-d0d07f786040","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}