{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-06T19:07:07.920189Z",
     "iopub.status.busy": "2025-02-06T19:07:07.919767Z",
     "iopub.status.idle": "2025-02-06T19:07:09.586065Z",
     "shell.execute_reply": "2025-02-06T19:07:09.584798Z",
     "shell.execute_reply.started": "2025-02-06T19:07:07.920149Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:07:13.986435Z",
     "iopub.status.busy": "2025-02-06T19:07:13.985921Z",
     "iopub.status.idle": "2025-02-06T19:07:28.130252Z",
     "shell.execute_reply": "2025-02-06T19:07:28.128771Z",
     "shell.execute_reply.started": "2025-02-06T19:07:13.986382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install NLTK\n",
    "!pip list | grep nltk\n",
    "! pip install -U kaleido\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  \n",
    "nltk.download('wordnet')  \n",
    "\n",
    "# Unzip per this stackoverflow: https://stackoverflow.com/questions/73849624/getting-error-while-submitting-notebook-on-kaggle-even-after-importing-nltk-libr\n",
    "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:07:35.034938Z",
     "iopub.status.busy": "2025-02-06T19:07:35.034169Z",
     "iopub.status.idle": "2025-02-06T19:08:00.236492Z",
     "shell.execute_reply": "2025-02-06T19:08:00.235251Z",
     "shell.execute_reply.started": "2025-02-06T19:07:35.034906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Clean text \n",
    "def clean_text(text):\n",
    "    lemma = WordNetLemmatizer() # lemmatizer\n",
    "    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text) # look into this tokenization\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens # lemmatize words and remove stopwords \n",
    "                if word not in stopwords.words(\"english\")]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Get the sentence embeddings for each course and user input with this function\n",
    "# First get the word embeddings and average them out for the sentence (aka course/input)\n",
    "# overall embedding\n",
    "\n",
    "def get_document_embedding(doc, model):\n",
    "    embeddings = [model.wv[word] for word in doc if word in model.wv] # Get individual embeddings into a list\n",
    "    # Consider implementing exception handling \n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0) \n",
    "    else:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "# Use previous functions to process user input into vector and use cosine \n",
    "# Similarity to find the most related courses\n",
    "def recommend_courses(user_input, document_embeddings, data, top_n=5):\n",
    "    cleaned_input = clean_text(user_input)\n",
    "    input_embedding = get_document_embedding(cleaned_input, model)\n",
    "    similarities = cosine_similarity([input_embedding], document_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n",
    "    return recommendations\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:08:04.505706Z",
     "iopub.status.busy": "2025-02-06T19:08:04.504907Z",
     "iopub.status.idle": "2025-02-06T19:08:10.981964Z",
     "shell.execute_reply": "2025-02-06T19:08:10.980711Z",
     "shell.execute_reply.started": "2025-02-06T19:08:04.505658Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Experiment with other data\n",
    "# edx, coursera, harvard, mit ocw\n",
    "# https://sparkbyexamples.com/pandas/pandas-read-multiple-csv-files/#:~:text=Load%20each%20file%20into%20individual,each%20file%20individually%20if%20needed.\n",
    "\n",
    "# Normalize/clean course data to the name, topic, link, text format for now\n",
    "\n",
    "dataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\n",
    "# print(dataMit.head())\n",
    "dataMit.columns = map(str.lower, dataMit.columns)\n",
    "dataMit.rename(columns={'name ': 'name'}, inplace=True)\n",
    "dataMit.rename(columns={'course link': 'link'}, inplace=True)\n",
    "dataMit['text'] = dataMit['name'] + \" \" + dataMit['topic'] \n",
    "dataMit['provider'] = 'Massachussets Institute of Technology'\n",
    "dataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "\n",
    "dataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\n",
    "# print(dataHarvard.head())\n",
    "dataHarvard.columns = map(str.lower, dataHarvard.columns)\n",
    "dataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\n",
    "dataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\n",
    "dataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic'] \n",
    "dataHarvard['provider'] = 'Harvard University'\n",
    "dataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "\n",
    "dataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\n",
    "# print(dataEdx.head())\n",
    "dataEdx.columns = map(str.lower, dataEdx.columns)\n",
    "dataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\n",
    "dataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\n",
    "dataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\n",
    "dataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "\n",
    "# Udemy\n",
    "dataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\n",
    "dataUdemy.columns = map(str.lower, dataUdemy.columns)\n",
    "dataUdemy.rename(columns={\n",
    "    'title': 'name',\n",
    "    'headline': 'topic',\n",
    "    'url': 'link',\n",
    "}, inplace=True)\n",
    "# only keep free courses\n",
    "dataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n",
    "# Since Udemy courses are user generated, filter only courses with rating over 4.5\n",
    "dataUdemy['provider'] = 'Udemy'\n",
    "dataUdemy = dataUdemy[dataUdemy['rating'] > 4.5 ]\n",
    "dataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\n",
    "dataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\n",
    "print(dataUdemy.head())\n",
    "\n",
    "\n",
    "# Coursera\n",
    "dataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\n",
    "dataCoursera.rename(columns={\n",
    "    'title': 'name',\n",
    "    'skills': 'topic',\n",
    "    'url': 'link',\n",
    "}, inplace=True)\n",
    "dataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\n",
    "dataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n",
    "\n",
    "dataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\n",
    "dataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]\n",
    "dataCoursera = dataCoursera.fillna(\"\") # Fill null values\n",
    "\n",
    "# print(dataCoursera.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:08:18.187417Z",
     "iopub.status.busy": "2025-02-06T19:08:18.186805Z",
     "iopub.status.idle": "2025-02-06T19:08:18.213014Z",
     "shell.execute_reply": "2025-02-06T19:08:18.211241Z",
     "shell.execute_reply.started": "2025-02-06T19:08:18.187363Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a Combined Dataframe\n",
    "data = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:08:27.090159Z",
     "iopub.status.busy": "2025-02-06T19:08:27.089760Z",
     "iopub.status.idle": "2025-02-06T19:09:00.316844Z",
     "shell.execute_reply": "2025-02-06T19:09:00.315888Z",
     "shell.execute_reply.started": "2025-02-06T19:08:27.090125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The list of tokenized sentences, ie our Corpus \n",
    "data['cleaned_text'] = data['text'].apply(clean_text) # Add clean text column to dataframe\n",
    "data['cleaned_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:11:48.673784Z",
     "iopub.status.busy": "2025-02-06T19:11:48.673422Z",
     "iopub.status.idle": "2025-02-06T19:11:50.019368Z",
     "shell.execute_reply": "2025-02-06T19:11:50.018258Z",
     "shell.execute_reply.started": "2025-02-06T19:11:48.673759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Corpus = list of tokenized sentences (already cleaned)\n",
    "corpus = data['cleaned_text'].tolist()\n",
    "print(corpus[0])\n",
    "\n",
    "# Train Word2Vec\n",
    "model = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=80,\n",
    "    window=5,       # Larger window for broader context\n",
    "    min_count=8,    # Ignore very rare words\n",
    "    workers=4,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:11:51.518758Z",
     "iopub.status.busy": "2025-02-06T19:11:51.518381Z",
     "iopub.status.idle": "2025-02-06T19:11:51.990537Z",
     "shell.execute_reply": "2025-02-06T19:11:51.989463Z",
     "shell.execute_reply.started": "2025-02-06T19:11:51.518731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get a list of the document embedding vector for each sentence in the cleaned text data. The indices will be aligned with the original course rows in dataframe\n",
    "document_embeddings = [get_document_embedding(doc, model)\n",
    "                      for doc in data['cleaned_text']]\n",
    "print(f'list of sentence vectors/sentences: {len(document_embeddings)}')\n",
    "print(f'each sentence has {document_embeddings[0].shape} dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T19:12:30.751980Z",
     "iopub.status.busy": "2025-02-06T19:12:30.751641Z",
     "iopub.status.idle": "2025-02-06T19:12:30.775844Z",
     "shell.execute_reply": "2025-02-06T19:12:30.774557Z",
     "shell.execute_reply.started": "2025-02-06T19:12:30.751954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# User interface (abstracted away)\n",
    "user_input = \"Lagrange Multipliers\"\n",
    "recommendations = recommend_courses(user_input, document_embeddings, data)\n",
    "recommendations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Notes\n",
    "\n",
    "* Original Source: https://www.kaggle.com/code/shtrausslearning/nlp-edx-course-recommendations\n",
    "* DeepSeek AI for original base which also calls the document embeddings for the input\n",
    "* I have added comments to better study and understand the code as a base to build off of\n",
    "* Instead of directly matching to a course index in the dataset which limits the use of the model\n",
    "\n",
    "# Todo (to better understand and be able to present on this topic)\n",
    "\n",
    "* Study cosine similarity\\\n",
    "* Word2Vec and stopwords/lemmatization\n",
    "* Coursera, futurelearn, udemy\n",
    "* Build a frontend for the app\n",
    "\n",
    "# Improvements\n",
    "\n",
    "reshaped corpus as list instead of series, increased context window, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (2.5.1+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.6.0)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (1.14.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/codespace/.local/lib/python3.12/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.0)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
      "Downloading sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m796.9/796.9 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed huggingface-hub-0.28.1 regex-2024.11.6 safetensors-0.5.2 sentence-transformers-3.4.1 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# Install Sentence-Transformers\n",
    "!pip install sentence-transformers\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Clean text function (unchanged)\n",
    "def clean_text(text):\n",
    "    lemma = WordNetLemmatizer()  # lemmatizer\n",
    "    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)  # tokenize\n",
    "    tokens = [lemma.lemmatize(word) for word in tokens  # lemmatize and remove stopwords\n",
    "              if word not in stopwords.words(\"english\")]\n",
    "    return \" \".join(tokens)  # Join tokens back into a sentence for SBERT\n",
    "\n",
    "# Load a pre-trained SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate sentence embeddings for all courses\n",
    "def get_sentence_embeddings(texts):\n",
    "    return model.encode(texts)  # SBERT generates embeddings directly\n",
    "\n",
    "# Use cosine similarity to find the most related courses\n",
    "def recommend_courses(user_input, document_embeddings, data, top_n=5):\n",
    "    cleaned_input = clean_text(user_input)\n",
    "    input_embedding = model.encode([cleaned_input])  # Generate embedding for user input\n",
    "    similarities = cosine_similarity(input_embedding, document_embeddings)[0]\n",
    "    top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n",
    "    return recommendations\n",
    "\n",
    "# Load and preprocess data (unchanged)\n",
    "dataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\n",
    "dataMit.columns = map(str.lower, dataMit.columns)\n",
    "dataMit.rename(columns={'name ': 'name'}, inplace=True)\n",
    "dataMit.rename(columns={'course link': 'link'}, inplace=True)\n",
    "dataMit['text'] = dataMit['name'] + \" \" + dataMit['topic']\n",
    "dataMit['provider'] = 'Massachussets Institute of Technology'\n",
    "dataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "dataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\n",
    "dataHarvard.columns = map(str.lower, dataHarvard.columns)\n",
    "dataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\n",
    "dataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\n",
    "dataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic']\n",
    "dataHarvard['provider'] = 'Harvard University'\n",
    "dataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "dataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\n",
    "dataEdx.columns = map(str.lower, dataEdx.columns)\n",
    "dataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\n",
    "dataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\n",
    "dataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\n",
    "dataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "dataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\n",
    "dataUdemy.columns = map(str.lower, dataUdemy.columns)\n",
    "dataUdemy.rename(columns={'title': 'name', 'headline': 'topic', 'url': 'link'}, inplace=True)\n",
    "dataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n",
    "dataUdemy['provider'] = 'Udemy'\n",
    "dataUdemy = dataUdemy[dataUdemy['rating'] > 4.5]\n",
    "dataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\n",
    "dataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\n",
    "\n",
    "dataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\n",
    "dataCoursera.rename(columns={'title': 'name', 'skills': 'topic', 'url': 'link'}, inplace=True)\n",
    "dataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\n",
    "dataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n",
    "dataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\n",
    "dataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]\n",
    "dataCoursera = dataCoursera.fillna(\"\")\n",
    "\n",
    "# Combine all datasets\n",
    "data = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\n",
    "\n",
    "# Clean text and generate embeddings\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "document_embeddings = get_sentence_embeddings(data['cleaned_text'].tolist())\n",
    "\n",
    "# Test the recommender\n",
    "user_input = \"Lagrange Multipliers\"\n",
    "recommendations = recommend_courses(user_input, document_embeddings, data)\n",
    "print(recommendations)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1868643,
     "sourceId": 3051937,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2423448,
     "sourceId": 5103339,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5456599,
     "sourceId": 9279572,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6138541,
     "sourceId": 9976645,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
