{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3051937,"sourceType":"datasetVersion","datasetId":1868643},{"sourceId":5103339,"sourceType":"datasetVersion","datasetId":2423448},{"sourceId":9279572,"sourceType":"datasetVersion","datasetId":5456599},{"sourceId":9976645,"sourceType":"datasetVersion","datasetId":6138541},{"sourceId":256574,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":204042,"modelId":225262}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Setup\n\n# Install NLTK and other packages\n!pip list | grep nltk\n! pip install -U kaleido\n!pip install sentence-transformers\n\nfrom sentence_transformers import SentenceTransformer\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re\nimport nltk\n\n\n\n\nnltk.download('punkt')  \nnltk.download('wordnet')  \n\n# Unzip per this stackoverflow: https://stackoverflow.com/questions/73849624/getting-error-while-submitting-notebook-on-kaggle-even-after-importing-nltk-libr\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"_uuid":"11872ad3-929a-453c-a9a8-212f3f47cff9","_cell_guid":"ad38c9bc-2362-4c5f-bb22-04197c45c913","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:26:27.723111Z","iopub.execute_input":"2025-03-05T16:26:27.723356Z","iopub.status.idle":"2025-03-05T16:27:26.334565Z","shell.execute_reply.started":"2025-03-05T16:26:27.723335Z","shell.execute_reply":"2025-03-05T16:27:26.333356Z"}},"outputs":[{"name":"stdout","text":"nltk                               3.2.4\nCollecting kaleido\n  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\nDownloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: kaleido\nSuccessfully installed kaleido-0.2.1\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nArchive:  /usr/share/nltk_data/corpora/wordnet.zip\n   creating: /usr/share/nltk_data/corpora/wordnet/\n  inflating: /usr/share/nltk_data/corpora/wordnet/lexnames  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adv.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.verb  \n  inflating: /usr/share/nltk_data/corpora/wordnet/cntlist.rev  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.adj  \n  inflating: /usr/share/nltk_data/corpora/wordnet/LICENSE  \n  inflating: /usr/share/nltk_data/corpora/wordnet/citation.bib  \n  inflating: /usr/share/nltk_data/corpora/wordnet/noun.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/verb.exc  \n  inflating: /usr/share/nltk_data/corpora/wordnet/README  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.sense  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/data.adv  \n  inflating: /usr/share/nltk_data/corpora/wordnet/index.noun  \n  inflating: /usr/share/nltk_data/corpora/wordnet/adj.exc  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Data cleaning and normalization\n\n# Normalize/clean course data to the name, topic, link, text format for now\n\ndataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\ndataMit.columns = map(str.lower, dataMit.columns)\ndataMit.rename(columns={'name ': 'name'}, inplace=True)\ndataMit.rename(columns={'course link': 'link'}, inplace=True)\ndataMit['text'] = dataMit['name'] + \" \" + dataMit['topic'] \ndataMit['provider'] = 'Massachussets Institute of Technology'\ndataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\ndataHarvard.columns = map(str.lower, dataHarvard.columns)\ndataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\ndataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\ndataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic'] \ndataHarvard['provider'] = 'Harvard University'\ndataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\ndataEdx.columns = map(str.lower, dataEdx.columns)\ndataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\ndataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\ndataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\ndataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\ndataUdemy.columns = map(str.lower, dataUdemy.columns)\ndataUdemy.rename(columns={\n    'title': 'name',\n    'headline': 'topic',\n    'url': 'link',\n}, inplace=True)\n# only keep free courses\ndataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n# Since Udemy courses are user generated, filter only courses with rating over 4.5\ndataUdemy['provider'] = 'Udemy'\ndataUdemy = dataUdemy[dataUdemy['rating'] > 4.5 ]\ndataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\ndataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\ndataCoursera.rename(columns={\n    'title': 'name',\n    'skills': 'topic',\n    'url': 'link',\n}, inplace=True)\ndataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\ndataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n\ndataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\ndataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]","metadata":{"_uuid":"96a00073-5fdf-4e9c-8ae4-a2aac33628dd","_cell_guid":"c5885aea-ad3a-4df3-8c01-65d9d1225cf1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:27:36.685389Z","iopub.execute_input":"2025-03-05T16:27:36.685754Z","iopub.status.idle":"2025-03-05T16:27:41.929450Z","shell.execute_reply.started":"2025-03-05T16:27:36.685721Z","shell.execute_reply":"2025-03-05T16:27:41.928464Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def clean_text(text):\n    lemma = WordNetLemmatizer() # lemmatizer\n    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n    text = text.lower()\n    tokens = word_tokenize(text) # look into this tokenization\n    tokens = [lemma.lemmatize(word) for word in tokens # lemmatize words and remove stopwords \n                if word not in stopwords.words(\"english\")]\n    return \" \".join(tokens) # SBERT rrequires joined tokens\n\n#Combine and clean data\ndata = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\ndata['cleaned_text'] = data['text'].apply(clean_text) # Add clean text column to dataframe\n\n# Drop non-english courses\nindices_to_drop = [index for index, row in data.iterrows() if bool(re.search(r'[^\\x00-\\x7F\\u2000-\\u206F\\u2600-\\u26FF\\u2700-\\u27BF]', str(row['text'])))]\ndata = data.drop(indices_to_drop)\n\ndata.head()","metadata":{"_uuid":"703c98e4-4486-4f12-a888-a3ffb4061644","_cell_guid":"5db69b4c-1811-4507-8f4c-41fa19d87acc","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:27:48.148697Z","iopub.execute_input":"2025-03-05T16:27:48.149011Z","iopub.status.idle":"2025-03-05T16:28:14.353534Z","shell.execute_reply.started":"2025-03-05T16:27:48.148989Z","shell.execute_reply":"2025-03-05T16:28:14.352657Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                                    name  \\\n26443                           Stock Market Foundations   \n26445  The Complete Course On Understanding Blockchai...   \n26446  Bitcoin or How I Learned to Stop Worrying and ...   \n26448  Blockchain cryptocurrency course 101 for absol...   \n26449  Trading Options For Consistent Returns: Option...   \n\n                                                   topic  \\\n26443  The Market isn't a Mystery, It’s a Playground....   \n26445  A Beginner's Guide to Authentic Knowledge on B...   \n26446  The definitive guide to understand what the bi...   \n26448  A complete guide to anyone who wants to really...   \n26449                     The Foundation For Consistency   \n\n                                                    link provider  \\\n26443  https://www.udemy.com/course/how-to-invest-in-...    Udemy   \n26445  https://www.udemy.com/course/understanding-blo...    Udemy   \n26446  https://www.udemy.com/course/bitcoin-or-how-i-...    Udemy   \n26448  https://www.udemy.com/course/blockchain-crypto...    Udemy   \n26449  https://www.udemy.com/course/trading-options-f...    Udemy   \n\n                                                    text  \\\n26443  Stock Market Foundations The Market isn't a My...   \n26445  The Complete Course On Understanding Blockchai...   \n26446  Bitcoin or How I Learned to Stop Worrying and ...   \n26448  Blockchain cryptocurrency course 101 for absol...   \n26449  Trading Options For Consistent Returns: Option...   \n\n                                            cleaned_text  \n26443  stock market foundation market isnt mystery pl...  \n26445  complete course understanding blockchain techn...  \n26446  bitcoin learned stop worrying love crypto defi...  \n26448  blockchain cryptocurrency course 101 absolute ...  \n26449  trading option consistent return option basic ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>topic</th>\n      <th>link</th>\n      <th>provider</th>\n      <th>text</th>\n      <th>cleaned_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>26443</th>\n      <td>Stock Market Foundations</td>\n      <td>The Market isn't a Mystery, It’s a Playground....</td>\n      <td>https://www.udemy.com/course/how-to-invest-in-...</td>\n      <td>Udemy</td>\n      <td>Stock Market Foundations The Market isn't a My...</td>\n      <td>stock market foundation market isnt mystery pl...</td>\n    </tr>\n    <tr>\n      <th>26445</th>\n      <td>The Complete Course On Understanding Blockchai...</td>\n      <td>A Beginner's Guide to Authentic Knowledge on B...</td>\n      <td>https://www.udemy.com/course/understanding-blo...</td>\n      <td>Udemy</td>\n      <td>The Complete Course On Understanding Blockchai...</td>\n      <td>complete course understanding blockchain techn...</td>\n    </tr>\n    <tr>\n      <th>26446</th>\n      <td>Bitcoin or How I Learned to Stop Worrying and ...</td>\n      <td>The definitive guide to understand what the bi...</td>\n      <td>https://www.udemy.com/course/bitcoin-or-how-i-...</td>\n      <td>Udemy</td>\n      <td>Bitcoin or How I Learned to Stop Worrying and ...</td>\n      <td>bitcoin learned stop worrying love crypto defi...</td>\n    </tr>\n    <tr>\n      <th>26448</th>\n      <td>Blockchain cryptocurrency course 101 for absol...</td>\n      <td>A complete guide to anyone who wants to really...</td>\n      <td>https://www.udemy.com/course/blockchain-crypto...</td>\n      <td>Udemy</td>\n      <td>Blockchain cryptocurrency course 101 for absol...</td>\n      <td>blockchain cryptocurrency course 101 absolute ...</td>\n    </tr>\n    <tr>\n      <th>26449</th>\n      <td>Trading Options For Consistent Returns: Option...</td>\n      <td>The Foundation For Consistency</td>\n      <td>https://www.udemy.com/course/trading-options-f...</td>\n      <td>Udemy</td>\n      <td>Trading Options For Consistent Returns: Option...</td>\n      <td>trading option consistent return option basic ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Initialize the model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Get a list of the document embedding vector for each sentence in the cleaned text data. The indices will be aligned with the original course rows in dataframe\ndocument_embeddings = model.encode(data['cleaned_text'].tolist())","metadata":{"_uuid":"413030c0-e87c-45cb-8889-5563c6bcd487","_cell_guid":"c6791a3a-7829-469a-906e-710b1bad80e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:28:19.235361Z","iopub.execute_input":"2025-03-05T16:28:19.235795Z","iopub.status.idle":"2025-03-05T16:28:32.200511Z","shell.execute_reply.started":"2025-03-05T16:28:19.235759Z","shell.execute_reply":"2025-03-05T16:28:32.199851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71be5a574a624958a617ee35f412928a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6fd9ab0198046c29d4f7e35d7563b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dabaf26f9df48ea8c65d469822ee641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff094b687299407a93f9b1aab4bafc3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b559b96b50241669fc2a1ae7334441e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cc8bf00eae341859c17936739a9ac54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a6d112e9244466995937f383814476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4870095cfc9944578560768d600753e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb2215264e54b8a8112aa9c5c5defbd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a3e171b61bd4bdf9d9ac70e6669ea64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6109af19c2e841d68fff395405ec6694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/164 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e759a1e5af49328c82b5275f622c40"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Export a csv with embeddings for fastapi\ndata = data[['name', 'topic', 'link', 'provider']]\ndata.to_csv('courses.csv', index=False)\nembeddings = pd.DataFrame(document_embeddings)\nembeddings.to_csv('embeddings.csv', index=False)","metadata":{"_uuid":"c71d32e2-0714-4827-8ce0-5241da0eb99b","_cell_guid":"1d152a10-a5f2-4f22-bed4-89003c49dbb7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-03-05T16:28:37.946995Z","iopub.execute_input":"2025-03-05T16:28:37.947311Z","iopub.status.idle":"2025-03-05T16:28:39.829861Z","shell.execute_reply.started":"2025-03-05T16:28:37.947286Z","shell.execute_reply":"2025-03-05T16:28:39.829072Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Use previous functions to process user input into vector and use cosine \n# Cosine Similarity to find the most related courses\ndef recommend_courses(user_input, document_embeddings, data, model, top_n=5):\n    cleaned_input = clean_text(user_input)\n    input_embedding = model.encode([cleaned_input]) # Model must be initialized\n    similarities = cosine_similarity(input_embedding, document_embeddings)[0]\n    top_indices = np.argsort(similarities)[-top_n:][::-1]\n    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n    return recommendations\n\n\nuser_input = \"Python Data Analytics\"\nrecommendations = recommend_courses(user_input, document_embeddings, data, model)\nrecommendations.head()","metadata":{"_uuid":"35733aa0-cfa5-4906-8acf-1cc3ce6e03d0","_cell_guid":"75e01ceb-702b-4b16-8229-f09e027434e9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-05T16:48:12.322797Z","iopub.execute_input":"2025-03-05T16:48:12.323141Z","iopub.status.idle":"2025-03-05T16:48:12.366358Z","shell.execute_reply.started":"2025-03-05T16:48:12.323120Z","shell.execute_reply":"2025-03-05T16:48:12.365546Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66f75fc14bf482e9b19f2f1f9087ec5"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                    name  \\\n65976  Marketing Analytics with Python: From Data to ...   \n61              Introduction to Data Science with Python   \n780                         Data Processing Using Python   \n309                                  Analytics in Python   \n138    Probability and Statistics in Data Science usi...   \n\n                                                   topic  \\\n65976                               Beginner to Advanced   \n61     Join Harvard University instructor Pavlos Prot...   \n780    Computer Programming, Python Programming, Comp...   \n309    Learn the fundamental of programming in Python...   \n138    Using Python, learn statistical and probabilis...   \n\n                                                    link  \\\n65976  https://www.udemy.com/course/python-for-market...   \n61     https://pll.harvard.edu/course/introduction-da...   \n780    https://www.coursera.org/learn/python-data-pro...   \n309       https://www.edx.org/course/analytics-in-python   \n138    https://www.edx.org/course/probability-and-sta...   \n\n                                            provider  \n65976                                          Udemy  \n61                                Harvard University  \n780                    Coursera - Nanjing University  \n309                        edX - Columbia University  \n138    edX - The University of California, San Diego  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>topic</th>\n      <th>link</th>\n      <th>provider</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>65976</th>\n      <td>Marketing Analytics with Python: From Data to ...</td>\n      <td>Beginner to Advanced</td>\n      <td>https://www.udemy.com/course/python-for-market...</td>\n      <td>Udemy</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>Introduction to Data Science with Python</td>\n      <td>Join Harvard University instructor Pavlos Prot...</td>\n      <td>https://pll.harvard.edu/course/introduction-da...</td>\n      <td>Harvard University</td>\n    </tr>\n    <tr>\n      <th>780</th>\n      <td>Data Processing Using Python</td>\n      <td>Computer Programming, Python Programming, Comp...</td>\n      <td>https://www.coursera.org/learn/python-data-pro...</td>\n      <td>Coursera - Nanjing University</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>Analytics in Python</td>\n      <td>Learn the fundamental of programming in Python...</td>\n      <td>https://www.edx.org/course/analytics-in-python</td>\n      <td>edX - Columbia University</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>Probability and Statistics in Data Science usi...</td>\n      <td>Using Python, learn statistical and probabilis...</td>\n      <td>https://www.edx.org/course/probability-and-sta...</td>\n      <td>edX - The University of California, San Diego</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Testing qwen workflow setup\n# For Qwen 1.5b inference\n\n# Imports + installs\n\nimport gc\nimport torch\nfrom IPython.display import display, Markdown, Latex, HTML\nimport time\nimport re\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n!pip install mistletoe\nimport mistletoe\n\ntorch.cuda.empty_cache()\ngc.collect()\n\ntorch.cuda.empty_cache()  # Clears unused cached memory\ntorch.cuda.ipc_collect()  # Collects unused memory\n\nprint(\"Using GPU:\", torch.cuda.get_device_name(0))\nprint(f'\\n\\nMemory Usage:')\nprint('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\nprint('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:16:04.799466Z","iopub.execute_input":"2025-03-05T17:16:04.799978Z","iopub.status.idle":"2025-03-05T17:16:09.595848Z","shell.execute_reply.started":"2025-03-05T17:16:04.799952Z","shell.execute_reply":"2025-03-05T17:16:09.595033Z"}},"outputs":[{"name":"stdout","text":"Collecting mistletoe\n  Downloading mistletoe-1.4.0-py3-none-any.whl.metadata (1.2 kB)\nDownloading mistletoe-1.4.0-py3-none-any.whl (51 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mistletoe\nSuccessfully installed mistletoe-1.4.0\nUsing GPU: Tesla P100-PCIE-16GB\n\n\nMemory Usage:\nAllocated: 0.1 GB\nCached:    0.1 GB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Inference LLM/setup\n\n\n# Load the Qwen 1.5b model\nmodel_name = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\"\nmodel_qwen = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"cuda\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Define the Qwen query function\ndef ask_model(system=\"You are a search query optimizer.\", prompt=\"Optimize this search query:\"):\n    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": prompt}]\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model_qwen.device)\n    generated_ids = model_qwen.generate(**model_inputs, max_new_tokens=3000, pad_token_id=tokenizer.eos_token_id)\n    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    return response\n\n# Example usage\nuser_input = \"Chaucer and Middle English Literature\"\noptimized_query = ask_model(prompt=f\"Optimize this search query: {user_input}\")\nrecommendations = recommend_courses(optimized_query, document_embeddings, data, model_bert)\nprint(recommendations)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T17:16:22.592902Z","iopub.execute_input":"2025-03-05T17:16:22.593192Z","iopub.status.idle":"2025-03-05T17:16:41.623379Z","shell.execute_reply.started":"2025-03-05T17:16:22.593170Z","shell.execute_reply":"2025-03-05T17:16:41.622200Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-205ead4de9d3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load the Qwen 1.5b model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-1.5b/2\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_qwen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4262\u001b[0m                     \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4263\u001b[0m                     \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4264\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4265\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[1;32m   4775\u001b[0m                                 )\n\u001b[1;32m   4776\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4777\u001b[0;31m                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   4778\u001b[0m                             \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4779\u001b[0m                             \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_quantized_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":10},{"cell_type":"markdown","source":"* https://huggingface.co/docs/transformers/en/model_doc/bert","metadata":{"_uuid":"79ca17d0-35bb-44de-953f-ba1de3a529d6","_cell_guid":"16751f0b-0b52-4c3f-ac9f-d971f41b0b87","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}