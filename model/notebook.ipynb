{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3051937,"sourceType":"datasetVersion","datasetId":1868643},{"sourceId":5103339,"sourceType":"datasetVersion","datasetId":2423448},{"sourceId":9279572,"sourceType":"datasetVersion","datasetId":5456599},{"sourceId":9976645,"sourceType":"datasetVersion","datasetId":6138541}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session test","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:07:07.919767Z","iopub.execute_input":"2025-02-06T19:07:07.920189Z","iopub.status.idle":"2025-02-06T19:07:09.586065Z","shell.execute_reply.started":"2025-02-06T19:07:07.920149Z","shell.execute_reply":"2025-02-06T19:07:09.584798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install NLTK\n!pip list | grep nltk\n! pip install -U kaleido\nimport nltk\n\nnltk.download('punkt')  \nnltk.download('wordnet')  \n\n# Unzip per this stackoverflow: https://stackoverflow.com/questions/73849624/getting-error-while-submitting-notebook-on-kaggle-even-after-importing-nltk-libr\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:07:13.985921Z","iopub.execute_input":"2025-02-06T19:07:13.986435Z","iopub.status.idle":"2025-02-06T19:07:28.130252Z","shell.execute_reply.started":"2025-02-06T19:07:13.986382Z","shell.execute_reply":"2025-02-06T19:07:28.128771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup\n\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom sklearn.metrics.pairwise import cosine_similarity \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport re\n\n# Clean text \ndef clean_text(text):\n    lemma = WordNetLemmatizer() # lemmatizer\n    text = re.sub(\"[^A-Za-z0-9 ]\", \"\", text)\n    text = text.lower()\n    tokens = word_tokenize(text) # look into this tokenization\n    tokens = [lemma.lemmatize(word) for word in tokens # lemmatize words and remove stopwords \n                if word not in stopwords.words(\"english\")]\n    return tokens\n\n\n# Get the sentence embeddings for each course and user input with this function\n# First get the word embeddings and average them out for the sentence (aka course/input)\n# overall embedding\n\ndef get_document_embedding(doc, model):\n    embeddings = [model.wv[word] for word in doc if word in model.wv] # Get individual embeddings into a list\n    # Consider implementing exception handling \n    if len(embeddings) > 0:\n        return np.mean(embeddings, axis=0) \n    else:\n        return np.zeros(model.vector_size)\n\n# Use previous functions to process user input into vector and use cosine \n# Similarity to find the most related courses\ndef recommend_courses(user_input, document_embeddings, data, top_n=5):\n    cleaned_input = clean_text(user_input)\n    input_embedding = get_document_embedding(cleaned_input, model)\n    similarities = cosine_similarity([input_embedding], document_embeddings)[0]\n    top_indices = np.argsort(similarities)[-top_n:][::-1]\n    recommendations = data.iloc[top_indices][['name', 'topic', 'link', 'provider']]\n    return recommendations\n\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:07:35.034169Z","iopub.execute_input":"2025-02-06T19:07:35.034938Z","iopub.status.idle":"2025-02-06T19:08:00.236492Z","shell.execute_reply.started":"2025-02-06T19:07:35.034906Z","shell.execute_reply":"2025-02-06T19:08:00.235251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Experiment with other data\n# edx, coursera, harvard, mit ocw\n# https://sparkbyexamples.com/pandas/pandas-read-multiple-csv-files/#:~:text=Load%20each%20file%20into%20individual,each%20file%20individually%20if%20needed.\n\n# Normalize/clean course data to the name, topic, link, text format for now\n\ndataMit = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/MIT ocw.csv\")\n# print(dataMit.head())\ndataMit.columns = map(str.lower, dataMit.columns)\ndataMit.rename(columns={'name ': 'name'}, inplace=True)\ndataMit.rename(columns={'course link': 'link'}, inplace=True)\ndataMit['text'] = dataMit['name'] + \" \" + dataMit['topic'] \ndataMit['provider'] = 'Massachussets Institute of Technology'\ndataMit = dataMit[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataHarvard = pd.read_csv(\"/kaggle/input/dataset-of-1200-coursera-courses/Harvard_university.csv\")\n# print(dataHarvard.head())\ndataHarvard.columns = map(str.lower, dataHarvard.columns)\ndataHarvard.rename(columns={'link to course': 'link', 'about': 'topic'}, inplace=True)\ndataHarvard = dataHarvard[dataHarvard['price'] == 'Free']\ndataHarvard['text'] = dataHarvard['name'] + \" \" + dataHarvard['topic'] \ndataHarvard['provider'] = 'Harvard University'\ndataHarvard = dataHarvard[['name', 'topic', 'link', 'provider', 'text']]\n\n\ndataEdx = pd.read_csv(\"/kaggle/input/edx-courses-dataset-2021/EdX.csv\")\n# print(dataEdx.head())\ndataEdx.columns = map(str.lower, dataEdx.columns)\ndataEdx[\"topic\"] = dataEdx['about'] + '. ' + dataEdx['course description']\ndataEdx[\"provider\"] = 'edX - ' + dataEdx['university']\ndataEdx['text'] = dataEdx['name'] + \" \" + dataEdx[\"topic\"]\ndataEdx = dataEdx[['name', 'topic', 'link', 'provider', 'text']]\n\n\n# Udemy\ndataUdemy = pd.read_csv(\"/kaggle/input/udemy-course-dataset-categories-ratings-and-trends/udemy_courses.csv\")\ndataUdemy.columns = map(str.lower, dataUdemy.columns)\ndataUdemy.rename(columns={\n    'title': 'name',\n    'headline': 'topic',\n    'url': 'link',\n}, inplace=True)\n# only keep free courses\ndataUdemy = dataUdemy[dataUdemy['is_paid'] == False]\n# Since Udemy courses are user generated, filter only courses with rating over 4.5\ndataUdemy['provider'] = 'Udemy'\ndataUdemy = dataUdemy[dataUdemy['rating'] > 4.5 ]\ndataUdemy['text'] = dataUdemy['name'] + \" \" + dataUdemy['topic']\ndataUdemy = dataUdemy[['name', 'topic', 'link', 'provider', 'text']]\nprint(dataUdemy.head())\n\n\n# Coursera\ndataCoursera = pd.read_csv(\"/kaggle/input/coursera-free-courses-dataset/coursera.csv\")\ndataCoursera.rename(columns={\n    'title': 'name',\n    'skills': 'topic',\n    'url': 'link',\n}, inplace=True)\ndataCoursera = dataCoursera[dataCoursera['price'] == 'Free']\ndataCoursera['text'] = dataCoursera['name'] + \" \" + np.where(pd.notna(dataCoursera['topic']), dataCoursera['topic'], \"\")\n\ndataCoursera['provider'] = 'Coursera - ' + dataCoursera['course_by']\ndataCoursera = dataCoursera[['name', 'topic', 'link', 'provider', 'text']]\ndataCoursera = dataCoursera.fillna(\"\") # Fill null values\n\n# print(dataCoursera.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:08:04.504907Z","iopub.execute_input":"2025-02-06T19:08:04.505706Z","iopub.status.idle":"2025-02-06T19:08:10.981964Z","shell.execute_reply.started":"2025-02-06T19:08:04.505658Z","shell.execute_reply":"2025-02-06T19:08:10.980711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Combined Dataframe\ndata = pd.concat([dataUdemy, dataMit, dataHarvard, dataEdx, dataCoursera])\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:08:18.186805Z","iopub.execute_input":"2025-02-06T19:08:18.187417Z","iopub.status.idle":"2025-02-06T19:08:18.213014Z","shell.execute_reply.started":"2025-02-06T19:08:18.187363Z","shell.execute_reply":"2025-02-06T19:08:18.211241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The list of tokenized sentences, ie our Corpus \ndata['cleaned_text'] = data['text'].apply(clean_text) # Add clean text column to dataframe\ndata['cleaned_text'].head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:08:27.089760Z","iopub.execute_input":"2025-02-06T19:08:27.090159Z","iopub.status.idle":"2025-02-06T19:09:00.316844Z","shell.execute_reply.started":"2025-02-06T19:08:27.090125Z","shell.execute_reply":"2025-02-06T19:09:00.315888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Corpus = list of tokenized sentences (already cleaned)\ncorpus = data['cleaned_text'].tolist()\nprint(corpus[0])\n\n# Train Word2Vec\nmodel = Word2Vec(\n    sentences=corpus,\n    vector_size=80,\n    window=5,       # Larger window for broader context\n    min_count=8,    # Ignore very rare words\n    workers=4,\n    epochs=10\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:11:48.673422Z","iopub.execute_input":"2025-02-06T19:11:48.673784Z","iopub.status.idle":"2025-02-06T19:11:50.019368Z","shell.execute_reply.started":"2025-02-06T19:11:48.673759Z","shell.execute_reply":"2025-02-06T19:11:50.018258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a list of the document embedding vector for each sentence in the cleaned text data. The indices will be aligned with the original course rows in dataframe\ndocument_embeddings = [get_document_embedding(doc, model)\n                      for doc in data['cleaned_text']]\nprint(f'list of sentence vectors/sentences: {len(document_embeddings)}')\nprint(f'each sentence has {document_embeddings[0].shape} dimensions')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:11:51.518381Z","iopub.execute_input":"2025-02-06T19:11:51.518758Z","iopub.status.idle":"2025-02-06T19:11:51.990537Z","shell.execute_reply.started":"2025-02-06T19:11:51.518731Z","shell.execute_reply":"2025-02-06T19:11:51.989463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# User interface (abstracted away)\nuser_input = \"Lagrange Multipliers\"\nrecommendations = recommend_courses(user_input, document_embeddings, data)\nrecommendations.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-06T19:12:30.751641Z","iopub.execute_input":"2025-02-06T19:12:30.751980Z","iopub.status.idle":"2025-02-06T19:12:30.775844Z","shell.execute_reply.started":"2025-02-06T19:12:30.751954Z","shell.execute_reply":"2025-02-06T19:12:30.774557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Important Notes\n\n* Original Source: https://www.kaggle.com/code/shtrausslearning/nlp-edx-course-recommendations\n* DeepSeek AI for original base which also calls the document embeddings for the input\n* I have added comments to better study and understand the code as a base to build off of\n* Instead of directly matching to a course index in the dataset which limits the use of the model\n\n# Todo (to better understand and be able to present on this topic)\n\n* Study cosine similarity\\\n* Word2Vec and stopwords/lemmatization\n* Coursera, futurelearn, udemy\n* Build a frontend for the app\n\n# Improvements\n\nreshaped corpus as list instead of series, increased context window, ","metadata":{}}]}